#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
@Project : gpt2_awq
@File    : other_utils.py
@Author  : Barry Allen
@Date    : 2025/3/17 11:40
@Desc    :
"""
import functools
import gc
import inspect
import time
import traceback
from typing import List, Dict, Optional, Tuple

import torch
from torch import nn

from yaspin import yaspin

from constant import CASES_FILE_NAME

RESET = "\033[0m"
GREEN = "\033[32;1m"
YELLOW = "\033[33;4m"

def spinner_run(text='Processing...', hide=False):
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            with yaspin(text=text, color="cyan") as spinner:
                start = time.time()
                try:
                    if hide: spinner.hide()
                    result = func(*args, **kwargs)
                    if hide: spinner.show()
                except Exception as e:
                    spinner.fail("ğŸ’¥ Failed")
                    traceback.print_exc()
                    exit(1)
                end = time.time()
                during = f'[{end-start:05.2f} s]'.replace('[0', '[ ')
                padding = ' ' * (64 - len(spinner.text) - len(result))
                spinner.text = f'{spinner.text}{YELLOW}{result}{RESET}{padding}{GREEN}{during}{RESET}'
                spinner.ok("âœ… Done")
                return result
        return wrapper
    return decorator

def visit_module(module: nn.Module):
    """
    å°†moduleé‡Œé¢å‚æ•°éå†ä¸ºfloat
    :param module:
    :return:
    """
    if not isinstance(module, nn.Linear) and hasattr(module, 'weight'):
        module.float()
    for name, child in module.named_children():
        visit_module(child)

def read_testcases_to_awq(file_path: str = CASES_FILE_NAME,
                          n_samples: int = 20,
                          max_seq_len: int = 16) -> List[torch.Tensor]:
    """
    è¯»å–æµ‹è¯•é›†æ¥è·å–éªŒè¯é›†åˆ
    :param file_path: è¿”å›æ–‡ä»¶è·¯å¾„
    :param n_samples: æœ€å¤§æ ·æœ¬æ•°é‡
    :param max_seq_len: æœ€å¤§å­—ç¬¦ä¸²é•¿åº¦
    :return: è¿”å›æ ·æœ¬çš„Tensor
    """
    ret: List[torch.Tensor] = []
    with open(file=file_path, mode='rt', encoding='utf-8') as f:
        while True:
            line:str = f.readline().strip()
            if not line: break
            simple_int: List[int] = list(map(lambda x: int(x), line.split(" ")))
            if len(simple_int) > max_seq_len:
                continue
            if len(ret) > n_samples:
                break
            ret.append(torch.tensor(simple_int))
    return ret

def get_best_device() -> torch.device:
    """
    è·å–æœ€ä½³çš„è®¾å¤‡
    :return:
    """
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")

def clear_memory(weight=None):
    """
    æ¸…ç†æƒé‡å†…å­˜
    :param weight:
    :return:
    """
    if weight is not None:
        del weight
    gc.collect()
    # æ ¹æ®è®¾å¤‡ç±»å‹æ‰§è¡Œç‰¹å®šçš„å†…å­˜æ¸…ç†
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    elif torch.mps.is_available():
        torch.mps.empty_cache()
    elif torch.backends.mps.is_available():
        pass

def get_named_linears(module: nn.Module) -> Dict[str, nn.Linear]:
    """
    è·å–çº¿æ€§å‡½æ•°åç§°
    :param module:
    :return:
    """
    return {name: m for name, m in module.named_modules() if isinstance(m, torch.nn.Linear)}

def exclude_layers_to_not_quantize(linear_layers: Dict[str, nn.Linear],
                                   modules_to_not_convert: Optional[List[str]] = None) -> Dict[str, nn.Linear]:
    """
    æ’é™¤ä¸éœ€è¦é‡åŒ–çš„å±‚ã€‚AWQè®ºæ–‡ä¸­æåˆ°ï¼Œå¹¶éæ‰€æœ‰æƒé‡éƒ½åŒç­‰é‡è¦ï¼Œä¿æŠ¤å…³é”®æƒé‡å¯¹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚
    è¿™ä¸ªå‡½æ•°å…è®¸ç”¨æˆ·æŒ‡å®šæŸäº›å±‚ä¿æŒåŸå§‹ç²¾åº¦ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ··åˆç²¾åº¦é‡åŒ–ï¼ˆè¿™åœ¨è®ºæ–‡ä¸­è¢«è®¤ä¸ºæ˜¯ç¡¬ä»¶æ•ˆç‡ä½ä¸‹çš„ï¼‰ã€‚
    :param linear_layers:  æ‰€æœ‰çº¿æ€§å±‚çš„å­—å…¸ï¼Œé”®ä¸ºå±‚åç§°
    :param modules_to_not_convert: åŒ…å«ä¸éœ€è¦é‡åŒ–çš„æ¨¡å—åç§°å…³é”®å­—åˆ—è¡¨
    :return:  è¿‡æ»¤åéœ€è¦è¿›è¡Œé‡åŒ–çš„çº¿æ€§å±‚å­—å…¸
    """
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä¸é‡åŒ–çš„æ¨¡å—ï¼Œè¿”å›æ‰€æœ‰å±‚
    if modules_to_not_convert is None:
        return linear_layers

    filtered_layers = {}
    for name, linear_layer in linear_layers.items():
        if not any(key in name for key in modules_to_not_convert):
            filtered_layers[name] = linear_layer
    return filtered_layers

def sanitize_kwargs(inputs_kwargs: Dict, module: nn.Module):
    """
    æ¸…ç†å¹¶è¿‡æ»¤è¾“å…¥å‚æ•°ï¼Œç§»é™¤æ¨¡å—å‰å‘ä¼ æ’­ä¸­ä¸æ”¯æŒçš„å‚æ•°ã€‚
    è¿™ä¸ªæ–¹æ³•çš„ä¸»è¦ç›®çš„æ˜¯ç¡®ä¿åœ¨ä¸åŒç‰ˆæœ¬çš„transformersåº“ä¹‹é—´ä¿æŒå…¼å®¹æ€§

    å·¥ä½œåŸç†ï¼š
    1. è·å–æ¨¡å—forwardæ–¹æ³•çš„å‚æ•°ç­¾å
    2. ä»…ä¿ç•™åœ¨ç­¾åä¸­å­˜åœ¨çš„å‚æ•°
    3. è¿”å›è¿‡æ»¤åçš„å‚æ•°å­—å…¸
    :param inputs_kwargs:  è¾“å…¥å‚æ•°å­—å…¸ï¼ŒåŒ…å«è¦ä¼ é€’ç»™æ¨¡å‹å±‚çš„æ‰€æœ‰å‚æ•°
    :param module:  ç›®æ ‡é‡åŒ–æ¨¡å—ï¼Œé€šå¸¸æ˜¯transformerä¸­çš„æŸä¸ªå­æ¨¡å—
    :return:  dict: ç»è¿‡æ¸…ç†çš„å‚æ•°å­—å…¸ï¼ŒåªåŒ…å«æ¨¡å—forwardæ–¹æ³•æ”¯æŒçš„å‚æ•°
    """
    module_signature = inspect.signature(module.forward).parameters  # è·å–æ¨¡å—forwardæ–¹æ³•çš„å‚æ•°ç­¾å
    sanitized_kwargs = {}  # åˆ›å»ºæ–°å­—å…¸å­˜å‚¨è¿‡æ»¤åçš„å‚æ•°
    for k, v in inputs_kwargs.items():  # éå†è¾“å…¥å‚æ•°ï¼Œåªä¿ç•™åœ¨æ¨¡å—ç­¾åä¸­å­˜åœ¨çš„å‚æ•°
        if k in module_signature:
            sanitized_kwargs[k] = v
    return sanitized_kwargs


def get_op_name(module: nn.Module, op: nn.Module):
    # get the name of the op relative to the module
    for name, m in module.named_modules():
        if m is op:
            return name
    raise ValueError(f"Cannot find op {op} in module {module}")

def get_op_by_name(module: nn.Module, op_name: str):
    """
    æ ¹æ®ç®—å­åç§°è·å–model
    :param module: æ¨¡å‹
    :param op_name:  ç®—å­åç§°
    :return:
    """
    # get the op by its name relative to the module
    for name, m in module.named_modules():
        if name == op_name:
            return m
    raise ValueError(f"Cannot find op {op_name} in module {module}")

def apply_scale(module, scales_list, input_feat_dict=None):
    """
    æ‰§è¡Œæœ€ä¼˜çš„scale
    :param module:
    :param scales_list:
    :param input_feat_dict:
    :return:
    """
    for prev_op_name, layer_names, scales in scales_list:
        prev_op = get_op_by_name(module, prev_op_name)
        layers = [get_op_by_name(module, name) for name in layer_names]

        best_device = get_best_device()
        prev_op.to(best_device)
        for layer in layers:
            layer.to(best_device)
        scales.to(best_device)
        if (
                isinstance(prev_op, torch.nn.Linear)
                and type(layers) == list
                and isinstance(layers[0], torch.nn.Linear)
        ):
            if len(layers) == 1:
                scale_fc_fc(prev_op, layers[0], scales)
            else:
                scale_fc_fcs(prev_op, layers, scales)
        elif (
                is_allowed_norms(prev_op)
                or "rmsnorm" in str(prev_op.__class__).lower()
        ):
            scale_ln_fcs(prev_op, layers, scales)

        elif is_allowed_act_fns(prev_op):
            scale_gelu_fc(prev_op, layers[0], scales)
        else:
            raise NotImplementedError(f"prev_op {type(prev_op)} not supported yet!")

        # apply the scaling to input feat if given; prepare it for clipping
        if input_feat_dict is not None:
            for layer_name in layer_names:
                # Skip the modules that are not quantized
                if layer_name in input_feat_dict:
                    inp = input_feat_dict[layer_name]
                    inp.div_(scales.view(1, -1).to(inp.device))

        prev_op.cpu()
        for layer in layers:
            layer.cpu()
        scales.cpu()

@torch.no_grad()
def scale_fc_fc(fc1: torch.nn.Linear, fc2: torch.nn.Linear, scales: torch.Tensor):
    """
    å¯¹ä¸¤ä¸ªç›¸é‚»çš„å…¨è¿æ¥å±‚åº”ç”¨AWQç¼©æ”¾ç­–ç•¥
    :param fc1: ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚
    :param fc2: ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚
    :param scales: ç¼©æ”¾å› å­å¼ é‡
    :return:
    """
    # ç¡®ä¿è¾“å…¥çš„å±‚æ˜¯nn.Linearç±»å‹
    assert isinstance(fc1, torch.nn.Linear)
    assert isinstance(fc2, torch.nn.Linear)

    # å°†scalesç§»åˆ°ä¸fc1æƒé‡ç›¸åŒçš„è®¾å¤‡ä¸Š
    scales = scales.to(fc1.weight.device)

    # å¯¹fc1æœ€åå‡ è¡Œæƒé‡è¿›è¡Œé€†å‘ç¼©æ”¾(é™¤ä»¥scales)
    # view(-1, 1)å°†scalesè½¬ä¸ºåˆ—å‘é‡ï¼Œç¡®ä¿å¹¿æ’­æœºåˆ¶æ­£ç¡®åº”ç”¨åˆ°æ¯è¡Œ
    fc1.weight[-scales.size(0):].div_(scales.view(-1, 1))
    # å¦‚æœfc1æœ‰åç½®é¡¹ï¼Œä¹Ÿéœ€è¦ç›¸åº”ç¼©æ”¾
    if fc1.bias is not None:
        fc1.bias.div_(scales.view(-1))

    # å¯¹fc2çš„æƒé‡è¿›è¡Œæ­£å‘ç¼©æ”¾(ä¹˜ä»¥scales)
    # view(1, -1)å°†scalesè½¬ä¸ºè¡Œå‘é‡ï¼Œç¡®ä¿å¹¿æ’­æœºåˆ¶æ­£ç¡®åº”ç”¨åˆ°æ¯åˆ—
    fc2.weight.mul_(scales.view(1, -1))

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ•°å€¼ä¸ç¨³å®š(NaN)çš„æƒ…å†µ
    for p in fc1.parameters():
        assert torch.isnan(p).sum() == 0
    for p in fc2.parameters():
        assert torch.isnan(p).sum() == 0

@torch.no_grad()
def scale_fc_fcs(fc1: torch.nn.Linear, fcs: List[torch.nn.Linear], scales: torch.Tensor):
    """
    å¯¹ä¸¤ä¸ªç›¸é‚»çš„å…¨è¿æ¥å±‚åº”ç”¨AWQç¼©æ”¾ç­–ç•¥
    :param fc1: ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚
    :param fcs: ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚åˆ—è¡¨
    :param scales:
    :return:
    """
    if not isinstance(fcs, list):
        fcs = [fcs]

    scales = scales.to(fc1.weight.device)

    fc1.weight[-scales.size(0):].div_(scales.view(-1, 1))
    if fc1.bias is not None:
        fc1.bias.div_(scales.view(-1))

    # å¯¹æ¯ä¸ªåç»­å±‚çš„æƒé‡è¿›è¡Œæ­£å‘ç¼©æ”¾(ä¹˜ä»¥scales)
    for fc in fcs:
        fc.weight.mul_(scales.view(1, -1))

    for p in fc1.parameters():
        assert torch.isnan(p).sum() == 0
    for fc in fcs:
        for p in fc.parameters():
            assert torch.isnan(p).sum() == 0

def is_allowed_norms(op):
    """
    æ˜¯å¦å…è®¸Norms
    :param op:
    :return:
    """
    if isinstance(op, torch.nn.LayerNorm):
        return True
    if any(t in str(type(op)) for t in ['LlamaRMSNorm', 'GemmaRMSNorm', 'CohereLayerNorm']):
        return True
    return False

@torch.no_grad()
def scale_ln_fcs(ln: torch.nn.Linear,
                 fcs: List[torch.nn.Linear],
                 scales: torch.Tensor):
    """
    å«æœ‰RMSNormä¸‹å®Œæˆçš„ç¼©æ”¾
    :param ln: 
    :param fcs: 
    :param scales: 
    :return: 
    """
    if not isinstance(fcs, list):
        fcs = [fcs]

    scales = scales.to(ln.weight.device)

    # GemmaRMSNorm is different from Llama's in that it multiplies
    # (1 + weight) to the output, instead of just weight.
    if 'GemmaRMSNorm' in str(type(ln)):
        ln.weight += 1
        ln.weight.div_(scales)
        ln.weight -= 1
    else:
        ln.weight.div_(scales)

    if hasattr(ln, "bias") and ln.bias is not None:
        ln.bias.div_(scales)

    for fc in fcs:
        fc.weight.mul_(scales.view(1, -1))

    for p in ln.parameters():
        assert torch.isnan(p).sum() == 0
    for fc in fcs:
        for p in fc.parameters():
            assert torch.isnan(p).sum() == 0

def is_allowed_act_fns(op):
    from transformers.activations import NewGELUActivation, PytorchGELUTanh, GELUActivation
    allowed_act_fns = [
        torch.nn.GELU,
        NewGELUActivation,
        PytorchGELUTanh,
        GELUActivation,
    ]
    return op in allowed_act_fns

@torch.no_grad()
def scale_gelu_fc(gelu, fc: torch.nn.Linear, scales: torch.Tensor):
    """
    GELU
    :param gelu:
    :param fc:
    :param scales:
    :return:
    """
    assert is_allowed_act_fns(gelu)
    assert isinstance(fc, torch.nn.Linear)

    fc.weight.mul_(scales.view(1, -1).to(fc.weight.device))

    for p in fc.parameters():
        assert torch.isnan(p).sum() == 0

@torch.no_grad()
def apply_clip(module, clip_list: Tuple[str, torch.Tensor]):
    for name, max_val in clip_list:
        layer: torch.nn.Linear = get_op_by_name(module, name)
        layer.to(get_best_device())
        max_val = max_val.to(layer.weight.device)
        org_shape = layer.weight.shape
        layer.weight.data = layer.weight.data.reshape(*max_val.shape[:2], -1)
        layer.weight.data = torch.clamp(layer.weight.data, -max_val, max_val)
        layer.weight.data = layer.weight.data.reshape(org_shape)
        layer.cpu()

def export_awq_gpt2_to_onnx(model: nn.Module,
                            save_path: str,
                            batch_size: int = 1,
                            sequence_length: int = 5):
    """
    å¯¼å‡ºGPT-2çš„onnxæ¨¡å‹
    :param model:
    :param save_path:
    :param batch_size:
    :param sequence_length:
    :return:
    """
    # 1. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # 2. å‡†å¤‡ç¤ºä¾‹è¾“å…¥
    dummy_input_ids: torch.Tensor = torch.randint(0, 6003, (batch_size, sequence_length))

    hidden_states: torch.Tensor = model.embedding(input_ids=dummy_input_ids)

    # 3. å®šä¹‰è¾“å…¥è¾“å‡ºåç§°
    input_names: List[str] = ["input_ids"]
    output_names: List[str] = ["output"]

    # 4. å¯¼å‡ºæ¨¡å‹
    torch.onnx.export(
        model,
        (hidden_states),  # æ¨¡å‹è¾“å…¥
        save_path,  # ä¿å­˜è·¯å¾„
        input_names=input_names,  # è¾“å…¥åç§°
        output_names=output_names,  # è¾“å‡ºåç§°
        do_constant_folding=True,  # è¿›è¡Œå¸¸é‡æŠ˜å ä¼˜åŒ–
        opset_version=20,  # ONNXç®—å­ç‰ˆæœ¬
        verbose=False,
        export_params=True,  # å¯¼å‡ºæ¨¡å‹å‚æ•°
    )


